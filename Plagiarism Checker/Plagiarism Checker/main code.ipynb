{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Plagiarism Detection using ML Models\n",
    "\n",
    "This notebook demonstrates enhanced plagiarism detection techniques using advanced ML models such as BERT and CodeBERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install Flask>=2.0.0 PyPDF2>=2.0.0 pdfplumber>=0.6.0 scikit-learn>=1.0.0 \\\n",
    "    sentence-transformers>=2.2.0 transformers>=4.0.0 torch>=1.10.0 \\\n",
    "    easyocr>=1.4.0 python-dotenv>=0.19.0 numpy>=1.20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import pdfplumber\n",
    "import logging\n",
    "import zipfile\n",
    "import tempfile\n",
    "import shutil\n",
    "import fnmatch\n",
    "from collections import defaultdict\n",
    "import difflib\n",
    "\n",
    "# ML-related imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Advanced models (might require installing)\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    SENTENCE_BERT_AVAILABLE = True\n",
    "    print(\"Sentence-BERT available\")\n",
    "except ImportError:\n",
    "    SENTENCE_BERT_AVAILABLE = False\n",
    "    print(\"Sentence-BERT not available\")\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    import torch\n",
    "    CODEBERT_AVAILABLE = True\n",
    "    print(\"CodeBERT dependencies available\")\n",
    "except ImportError:\n",
    "    CODEBERT_AVAILABLE = False\n",
    "    print(\"CodeBERT dependencies not available\")\n",
    "    \n",
    "try:\n",
    "    import easyocr\n",
    "    OCR_AVAILABLE = True\n",
    "    print(\"EasyOCR available\")\n",
    "except ImportError:\n",
    "    OCR_AVAILABLE = False\n",
    "    print(\"EasyOCR not available\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Comparison with Sentence-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize Sentence-BERT model if available\n",
    "if SENTENCE_BERT_AVAILABLE:\n",
    "    try:\n",
    "        sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"Sentence-BERT model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Sentence-BERT model: {e}\")\n",
    "        SENTENCE_BERT_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text by converting to lowercase, removing special characters, and extra whitespace\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def get_text_similarity_tfidf(text1, text2):\n",
    "    \"\"\"Calculate cosine similarity between two text documents using TF-IDF\"\"\"\n",
    "    try:\n",
    "        # Preprocess the texts\n",
    "        preprocessed_text1 = preprocess_text(text1)\n",
    "        preprocessed_text2 = preprocess_text(text2)\n",
    "        \n",
    "        # Create TF-IDF vectorizer and transform documents\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform([preprocessed_text1, preprocessed_text2])\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "        return similarity\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating text similarity with TF-IDF: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def get_text_similarity_bert(text1, text2):\n",
    "    \"\"\"Calculate semantic similarity using Sentence-BERT\"\"\"\n",
    "    if not SENTENCE_BERT_AVAILABLE:\n",
    "        return get_text_similarity_tfidf(text1, text2)\n",
    "    \n",
    "    try:\n",
    "        # Truncate very long texts to avoid memory issues\n",
    "        text1 = text1[:10000] if len(text1) > 10000 else text1\n",
    "        text2 = text2[:10000] if len(text2) > 10000 else text2\n",
    "        \n",
    "        # Generate embeddings\n",
    "        emb1 = sentence_model.encode(text1, convert_to_tensor=True)\n",
    "        emb2 = sentence_model.encode(text2, convert_to_tensor=True)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = util.pytorch_cos_sim(emb1, emb2).item()\n",
    "        return similarity\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating text similarity with BERT: {e}\")\n",
    "        # Fallback to traditional method\n",
    "        return get_text_similarity_tfidf(text1, text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PDF Text Extraction with EasyOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize EasyOCR if available\n",
    "if OCR_AVAILABLE:\n",
    "    try:\n",
    "        ocr_reader = easyocr.Reader(['en'])  # Initialize once (slow)\n",
    "        print(\"EasyOCR initialized successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing EasyOCR: {e}\")\n",
    "        OCR_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text content from a PDF file using PyPDF2 and pdfplumber as fallback\"\"\"\n",
    "    try:\n",
    "        # First try with PyPDF2\n",
    "        text = \"\"\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page_text = pdf_reader.pages[page_num].extract_text() or \"\"\n",
    "                text += page_text + \"\\n\"\n",
    "        \n",
    "        # If PyPDF2 fails to extract meaningful text, try pdfplumber\n",
    "        if text.strip() == \"\":\n",
    "            text = \"\"\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    page_text = page.extract_text() or \"\"\n",
    "                    text += page_text + \"\\n\"\n",
    "        \n",
    "        # If both text extraction methods fail, it might be a scanned PDF, try OCR\n",
    "        if text.strip() == \"\" and OCR_AVAILABLE:\n",
    "            try:\n",
    "                text = extract_text_with_easyocr(pdf_path)\n",
    "                print(f\"Used OCR to extract text from {pdf_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"OCR extraction failed: {e}\")\n",
    "                \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_with_easyocr(file_path):\n",
    "    \"\"\"Extract text from scanned PDFs/images using EasyOCR\"\"\"\n",
    "    if not OCR_AVAILABLE:\n",
    "        return \"\"\n",
    "        \n",
    "    try:\n",
    "        results = ocr_reader.readtext(file_path, paragraph=True)\n",
    "        return \"\\n\".join([res[1] for res in results])\n",
    "    except Exception as e:\n",
    "        print(f\"EasyOCR failed: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Code Comparison with CodeBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize CodeBERT model if available\n",
    "if CODEBERT_AVAILABLE:\n",
    "    try:\n",
    "        code_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "        code_model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "        print(\"CodeBERT model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CodeBERT model: {e}\")\n",
    "        CODEBERT_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def preprocess_code(code, language):\n",
    "    \"\"\"Preprocess code for comparison\"\"\"\n",
    "    # Remove comments based on language\n",
    "    if language == 'python':\n",
    "        # Remove Python-style comments\n",
    "        code = re.sub(r'#.*', '', code)\n",
    "        # Remove docstrings\n",
    "        code = re.sub(r'\"\"\"[\\s\\S]*?\"\"\"', '', code)\n",
    "        code = re.sub(r\"'''[\\s\\S]*?'''\", '', code)\n",
    "    elif language in ['java', 'javascript', 'c', 'cpp', 'csharp']:\n",
    "        # Remove C-style comments\n",
    "        code = re.sub(r'//.*', '', code)\n",
    "        code = re.sub(r'/\\*[\\s\\S]*?\\*/', '', code)\n",
    "    \n",
    "    # Remove excess whitespace\n",
    "    code = re.sub(r'\\s+', ' ', code).strip()\n",
    "    \n",
    "    return code\n",
    "\n",
    "def embed_code(code, max_length=512):\n",
    "    \"\"\"Generate CodeBERT embeddings for code snippets\"\"\"\n",
    "    if not CODEBERT_AVAILABLE:\n",
    "        raise ValueError(\"CodeBERT model not available\")\n",
    "        \n",
    "    try:\n",
    "        inputs = code_tokenizer(code, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "        with torch.no_grad():\n",
    "            outputs = code_model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating CodeBERT embeddings: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stylometry Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_stylometric_features(text):\n",
    "    \"\"\"Extract writing style features\"\"\"\n",
    "    # Handle empty text\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return {\n",
    "            'avg_word_length': 0,\n",
    "            'punctuation_density': 0,\n",
    "            'avg_sentence_length': 0,\n",
    "            'function_word_ratio': 0,\n",
    "            'uppercase_ratio': 0\n",
    "        }\n",
    "    \n",
    "    # Tokenize\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]', text) if s.strip()]\n",
    "    \n",
    "    # Function words (common words that don't carry strong meaning)\n",
    "    function_words = {\n",
    "        'the', 'a', 'an', 'and', 'or', 'but', 'if', 'while', 'of', 'at',\n",
    "        'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through',\n",
    "        'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up',\n",
    "        'down', 'in', 'out', 'on', 'off', 'over', 'under', 'he', 'she', 'it',\n",
    "        'they', 'we', 'who', 'what', 'where', 'when', 'why', 'how'\n",
    "    }\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_chars = sum(len(w) for w in words) if words else 0\n",
    "    total_punctuation = sum(1 for c in text if c in ',.;:!?\"()')\n",
    "    function_word_count = sum(1 for w in words if w in function_words)\n",
    "    uppercase_count = sum(1 for c in text if c.isupper())\n",
    "    \n",
    "    avg_word_length = total_chars / len(words) if words else 0\n",
    "    punctuation_density = total_punctuation / len(text) if text else 0\n",
    "    avg_sentence_length = len(words) / len(sentences) if sentences else 0\n",
    "    function_word_ratio = function_word_count / len(words) if words else 0\n",
    "    uppercase_ratio = uppercase_count / len(text) if text else 0\n",
    "    \n",
    "    return {\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'punctuation_density': punctuation_density,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'function_word_ratio': function_word_ratio,\n",
    "        'uppercase_ratio': uppercase_ratio\n",
    "    }\n",
    "\n",
    "def get_stylometric_similarity(text1, text2):\n",
    "    \"\"\"Calculate similarity between stylometric features of two texts\"\"\"\n",
    "    features1 = extract_stylometric_features(text1)\n",
    "    features2 = extract_stylometric_features(text2)\n",
    "    \n",
    "    # Calculate normalized Euclidean distance between feature vectors\n",
    "    squared_diff_sum = 0\n",
    "    for key in features1:\n",
    "        # Skip if feature is 0 in both texts\n",
    "        if features1[key] == 0 and features2[key] == 0:\n",
    "            continue\n",
    "            \n",
    "        max_val = max(abs(features1[key]), abs(features2[key]))\n",
    "        if max_val > 0:  # Avoid division by zero\n",
    "            norm1 = features1[key] / max_val\n",
    "            norm2 = features2[key] / max_val\n",
    "            squared_diff_sum += (norm1 - norm2) ** 2\n",
    "    \n",
    "    # Convert distance to similarity (1 = identical, 0 = completely different)\n",
    "    distance = np.sqrt(squared_diff_sum)\n",
    "    similarity = 1 / (1 + distance)  # Transform using sigmoid-like function\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Finding Matching Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def find_matching_sections(text1, text2, min_length=40, max_sections=10):\n",
    "    \"\"\"Find and return the matching sections between two texts\"\"\"\n",
    "    try:\n",
    "        # Split texts into sentences\n",
    "        sentences1 = re.split(r'(?<=[.!?])\\s+', text1)\n",
    "        sentences2 = re.split(r'(?<=[.!?])\\s+', text2)\n",
    "        \n",
    "        # Find matching sections using difflib\n",
    "        matcher = difflib.SequenceMatcher(None, sentences1, sentences2)\n",
    "        matching_blocks = matcher.get_matching_blocks()\n",
    "        \n",
    "        matched_sections = []\n",
    "        for match in matching_blocks:\n",
    "            i, j, size = match\n",
    "            if size > 0:\n",
    "                # Extract matching text sections\n",
    "                matched_text1 = ' '.join(sentences1[i:i+size])\n",
    "                matched_text2 = ' '.join(sentences2[j:j+size])\n",
    "                \n",
    "                # Only include if they're substantial enough\n",
    "                if len(matched_text1) >= min_length:\n",
    "                    # Calculate similarity for this section\n",
    "                    try:\n",
    "                        if SENTENCE_BERT_AVAILABLE:\n",
    "                            section_similarity = get_text_similarity_bert(matched_text1, matched_text2)\n",
    "                        else:\n",
    "                            section_similarity = get_text_similarity_tfidf(matched_text1, matched_text2)\n",
    "                    except Exception:\n",
    "                        section_similarity = get_text_similarity_tfidf(matched_text1, matched_text2)\n",
    "                    \n",
    "                    matched_sections.append({\n",
    "                        'file1_text': matched_text1,\n",
    "                        'file2_text': matched_text2,\n",
    "                        'similarity': section_similarity\n",
    "                    })\n",
    "        \n",
    "        # Sort by similarity (highest first) and limit the number of sections\n",
    "        matched_sections.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        return matched_sections[:max_sections]\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding matching sections: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Combined Plagiarism Detection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_plagiarism(file1_path, file2_path, comparison_type, use_advanced_models=True):\n",
    "    \"\"\"Main function to detect plagiarism between different file types\"\"\"\n",
    "    try:\n",
    "        results = {}\n",
    "        \n",
    "        # Text comparison\n",
    "        if comparison_type == 'text_text':\n",
    "            # Read files\n",
    "            with open(file1_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                text1 = f.read()\n",
    "            with open(file2_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                text2 = f.read()\n",
    "            \n",
    "            # Calculate similarity using BERT if available\n",
    "            if use_advanced_models and SENTENCE_BERT_AVAILABLE:\n",
    "                semantic_similarity = get_text_similarity_bert(text1, text2)\n",
    "            else:\n",
    "                semantic_similarity = get_text_similarity_tfidf(text1, text2)\n",
    "            \n",
    "            # Get stylometric similarity\n",
    "            style_similarity = get_stylometric_similarity(text1, text2)\n",
    "            \n",
    "            # Combine similarities (80% semantic, 20% style)\n",
    "            combined_similarity = 0.8 * semantic_similarity + 0.2 * style_similarity\n",
    "            \n",
    "            # Find matching sections\n",
    "            matched_sections = find_matching_sections(text1, text2)\n",
    "            \n",
    "            results = {\n",
    "                'similarity': combined_similarity,\n",
    "                'semantic_similarity': semantic_similarity,\n",
    "                'style_similarity': style_similarity,\n",
    "                'matched_sections': matched_sections\n",
    "            }\n",
    "            \n",
    "        # PDF comparison\n",
    "        elif comparison_type == 'pdf_pdf':\n",
    "            # Extract text from PDFs\n",
    "            text1 = extract_text_from_pdf(file1_path)\n",
    "            text2 = extract_text_from_pdf(file2_path)\n",
    "            \n",
    "            # If extraction failed, return zero similarity\n",
    "            if not text1 or not text2:\n",
    "                return {\n",
    "                    'similarity': 0.0,\n",
    "                    'matched_sections': [],\n",
    "                    'error': 'Text extraction from PDF failed'\n",
    "                }\n",
    "            \n",
    "            # Calculate similarity using BERT if available\n",
    "            if use_advanced_models and SENTENCE_BERT_AVAILABLE:\n",
    "                similarity = get_text_similarity_bert(text1, text2)\n",
    "            else:\n",
    "                similarity = get_text_similarity_tfidf(text1, text2)\n",
    "                \n",
    "            # Find matching sections\n",
    "            matched_sections = find_matching_sections(text1, text2)\n",
    "            \n",
    "            results = {\n",
    "                'similarity': similarity,\n",
    "                'matched_sections': matched_sections\n",
    "            }\n",
    "            \n",
    "        # Text vs PDF comparison\n",
    "        elif comparison_type == 'text_pdf':\n",
    "            # Read text file\n",
    "            with open(file1_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                text1 = f.read()\n",
    "                \n",
    "            # Extract text from PDF\n",
    "            text2 = extract_text_from_pdf(file2_path)\n",
    "            \n",
    "            # If extraction failed, return zero similarity\n",
    "            if not text2:\n",
    "                return {\n",
    "                    'similarity': 0.0,\n",
    "                    'matched_sections': [],\n",
    "                    'error': 'Text extraction from PDF failed'\n",
    "                }\n",
    "            \n",
    "            # Calculate similarity using BERT if available\n",
    "            if use_advanced_models and SENTENCE_BERT_AVAILABLE:\n",
    "                similarity = get_text_similarity_bert(text1, text2)\n",
    "            else:\n",
    "                similarity = get_text_similarity_tfidf(text1, text2)\n",
    "                \n",
    "            # Find matching sections\n",
    "            matched_sections = find_matching_sections(text1, text2)\n",
    "            \n",
    "            results = {\n",
    "                'similarity': similarity,\n",
    "                'matched_sections': matched_sections\n",
    "            }\n",
    "            \n",
    "        # GitHub repositories comparison\n",
    "        elif comparison_type == 'github_github':\n",
    "            return compare_github_repos(file1_path, file2_path, use_advanced_models)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Invalid comparison type: {comparison_type}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in plagiarism detection: {e}\")\n",
    "        return {\n",
    "            'error': str(e),\n",
    "            'similarity': 0.0,\n",
    "            'matched_sections': []\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. GitHub Repositories Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_zip(zip_path, extract_to=None):\n",
    "    \"\"\"Extract a ZIP file to a temporary directory\"\"\"\n",
    "    try:\n",
    "        if extract_to is None:\n",
    "            extract_to = tempfile.mkdtemp()\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        \n",
    "        return extract_to\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting ZIP file {zip_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_language(filepath):\n",
    "    \"\"\"Detect programming language based on file extension\"\"\"\n",
    "    extension_map = {\n",
    "        '.py': 'python',\n",
    "        '.java': 'java',\n",
    "        '.js': 'javascript',\n",
    "        '.html': 'html',\n",
    "        '.css': 'css',\n",
    "        '.c': 'c',\n",
    "        '.cpp': 'cpp',\n",
    "        '.h': 'c',\n",
    "        '.hpp': 'cpp',\n",
    "        '.cs': 'csharp',\n",
    "        '.php': 'php',\n",
    "        '.rb': 'ruby',\n",
    "        '.go': 'go',\n",
    "        '.ts': 'typescript'\n",
    "    }\n",
    "    _, ext = os.path.splitext(filepath)\n",
    "    return extension_map.get(ext.lower(), None)\n",
    "\n",
    "def should_ignore_file(filepath):\n",
    "    \"\"\"Check if a file should be ignored in the comparison\"\"\"\n",
    "    ignore_patterns = [\n",
    "        '*.git*',\n",
    "        '*.DS_Store',\n",
    "        '*__pycache__*',\n",
    "        '*.pyc',\n",
    "        'node_modules/*',\n",
    "        'venv/*',\n",
    "        '*.jar',\n",
    "        '*.class'\n",
    "    ]\n",
    "    \n",
    "    for pattern in ignore_patterns:\n",
    "        if fnmatch.fnmatch(filepath, pattern):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def compare_code_files(file1_path, file2_path, use_codebert=True):\n",
    "    \"\"\"Compare two source code files\"\"\"\n",
    "    try:\n",
    "        # Detect language\n",
    "        file1_language = detect_language(file1_path)\n",
    "        file2_language = detect_language(file2_path)\n",
    "        \n",
    "        # If languages don't match, similarity is likely low\n",
    "        if file1_language != file2_language:\n",
    "            return 0.1, []\n",
    "        \n",
    "        # Read file contents\n",
    "        with open(file1_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            code1 = f.read()\n",
    "        \n",
    "        with open(file2_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            code2 = f.read()\n",
    "        \n",
    "        # Preprocess code\n",
    "        processed_code1 = preprocess_code(code1, file1_language)\n",
    "        processed_code2 = preprocess_code(code2, file2_language)\n",
    "        \n",
    "        # Calculate similarity using CodeBERT if available\n",
    "        if use_codebert and CODEBERT_AVAILABLE:\n",
    "            try:\n",
    "                # Generate embeddings\n",
    "                emb1 = embed_code(processed_code1)\n",
    "                emb2 = embed_code(processed_code2)\n",
    "                \n",
    "                # Calculate cosine similarity\n",
    "                similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "            except Exception as e:\n",
    "                print(f\"CodeBERT comparison failed: {e}\")\n",
    "                if SENTENCE_BERT_AVAILABLE:\n",
    "                    similarity = get_text_similarity_bert(processed_code1, processed_code2)\n",
    "                else:\n",
    "                    similarity = get_text_similarity_tfidf(processed_code1, processed_code2)\n",
    "        elif SENTENCE_BERT_AVAILABLE:\n",
    "            similarity = get_text_similarity_bert(processed_code1, processed_code2)\n",
    "        else:\n",
    "            similarity = get_text_similarity_tfidf(processed_code1, processed_code2)\n",
    "        \n",
    "        # Find matching sections if similarity is significant\n",
    "        matched_sections = []\n",
    "        if similarity > 0.3:\n",
    "            matched_sections = find_matching_sections(\n",
    "                code1, code2, min_length=20, max_sections=5\n",
    "            )\n",
    "        \n",
    "        return similarity, matched_sections\n",
    "    except Exception as e:\n",
    "        print(f\"Error comparing code files: {e}\")\n",
    "        return 0.0, []\n",
    "\n",
    "def compare_github_repos(repo1_zip, repo2_zip, use_advanced_models=True):\n",
    "    \"\"\"Compare two GitHub repositories\"\"\"\n",
    "    temp_dirs = []\n",
    "    try:\n",
    "        # Extract ZIPs\n",
    "        repo1_dir = extract_zip(repo1_zip)\n",
    "        repo2_dir = extract_zip(repo2_zip)\n",
    "        \n",
    "        if not repo1_dir or not repo2_dir:\n",
    "            return {\n",
    "                'similarity': 0.0,\n",
    "                'matched_sections': [],\n",
    "                'error': 'Failed to extract ZIP files'\n",
    "            }\n",
    "        \n",
    "        temp_dirs.extend([repo1_dir, repo2_dir])\n",
    "        \n",
    "        # Get all code files recursively\n",
    "        repo1_files = []\n",
    "        for root, _, files in os.walk(repo1_dir):\n",
    "            for file in files:\n",
    "                filepath = os.path.join(root, file)\n",
    "                rel_path = os.path.relpath(filepath, repo1_dir)\n",
    "                if not should_ignore_file(rel_path) and detect_language(filepath):\n",
    "                    repo1_files.append(filepath)\n",
    "        \n",
    "        repo2_files = []\n",
    "        for root, _, files in os.walk(repo2_dir):\n",
    "            for file in files:\n",
    "                filepath = os.path.join(root, file)\n",
    "                rel_path = os.path.relpath(filepath, repo2_dir)\n",
    "                if not should_ignore_file(rel_path) and detect_language(filepath):\n",
    "                    repo2_files.append(filepath)\n",
    "        \n",
    "        if not repo1_files or not repo2_files:\n",
    "            return {\n",
    "                'similarity': 0.0,\n",
    "                'matched_sections': [],\n",
    "                'error': 'No source code files found in repositories'\n",
    "            }\n",
    "        \n",
    "        # Compare files\n",
    "        file_similarities = []\n",
    "        best_matches = []\n",
    "        \n",
    "        # Limit the number of comparisons\n",
    "        max_comparisons = min(len(repo1_files) * len(repo2_files), 100)\n",
    "        comparisons_done = 0\n",
    "        \n",
    "        for file1 in repo1_files:\n",
    "            file1_rel = os.path.relpath(file1, repo1_dir)\n",
    "            file1_lang = detect_language(file1)\n",
    "            \n",
    "            for file2 in repo2_files:\n",
    "                file2_rel = os.path.relpath(file2, repo2_dir)\n",
    "                file2_lang = detect_language(file2)\n",
    "                \n",
    "                # Skip if languages don't match\n",
    "                if file1_lang != file2_lang:\n",
    "                    continue\n",
    "                    \n",
    "                # Count comparisons\n",
    "                comparisons_done += 1\n",
    "                if comparisons_done > max_comparisons:\n",
    "                    break\n",
    "                \n",
    "                # Compare files\n",
    "                similarity, sections = compare_code_files(file1, file2, use_advanced_models)\n",
    "                \n",
    "                if similarity > 0.0:\n",
    "                    file_similarities.append(similarity)\n",
    "                    \n",
    "                    # Only keep high similarity matches\n",
    "                    if similarity > 0.7:\n",
    "                        best_matches.append({\n",
    "                            'file1': file1_rel,\n",
    "                            'file2': file2_rel,\n",
    "                            'similarity': similarity,\n",
    "                            'matched_sections': sections\n",
    "                        })\n",
    "            \n",
    "            if comparisons_done > max_comparisons:\n",
    "                break\n",
    "        \n",
    "        # Calculate overall similarity\n",
    "        if not file_similarities:\n",
    "            return {\n",
    "                'similarity': 0.0,\n",
    "                'matched_sections': [],\n",
    "                'message': 'No similar files found'\n",
    "            }\n",
    "        \n",
    "        overall_similarity = sum(file_similarities) / len(file_similarities)\n",
    "        \n",
    "        # Prepare matched sections\n",
    "        matched_sections = []\n",
    "        for match in sorted(best_matches, key=lambda x: x['similarity'], reverse=True)[:5]:\n",
    "            for section in match['matched_sections']:\n",
    "                matched_sections.append({\n",
    "                    'file1_text': f\"File: {match['file1']}\\n{section['file1_text']}\",\n",
    "                    'file2_text': f\"File: {match['file2']}\\n{section['file2_text']}\",\n",
    "                    'similarity': section['similarity']\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            'similarity': overall_similarity,\n",
    "            'matched_sections': matched_sections,\n",
    "            'matching_files': len(best_matches),\n",
    "            'total_files_compared': comparisons_done\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error comparing GitHub repositories: {e}\")\n",
    "        return {\n",
    "            'error': str(e),\n",
    "            'similarity': 0.0,\n",
    "            'matched_sections': []\n",
    "        }\n",
    "    \n",
    "    finally:\n",
    "        # Clean up temp directories\n",
    "        for temp_dir in temp_dirs:\n",
    "            if temp_dir and os.path.exists(temp_dir):\n",
    "                shutil.rmtree(temp_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Testing and Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_test_files():\n",
    "    \"\"\"Create sample files for testing\"\"\"\n",
    "    os.makedirs('test_files', exist_ok=True)\n",
    "    \n",
    "    # Create original text\n",
    "    original_text = \"\"\"\n",
    "    Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to natural intelligence \n",
    "    displayed by animals including humans. AI research has been defined as the field of study of intelligent agents, \n",
    "    which refers to any system that perceives its environment and takes actions that maximize its chance of achieving \n",
    "    its goals. The term \"artificial intelligence\" had previously been used to describe machines that mimic and display \n",
    "    human cognitive skills that are associated with the human mind, such as learning and problem-solving. This definition \n",
    "    has since been rejected by major AI researchers who now describe AI in terms of rationality and acting rationally, \n",
    "    which does not limit how intelligence can be articulated.\n",
    "    \n",
    "    AI applications include advanced web search engines, recommendation systems, content creation, speech recognition, \n",
    "    machine translation, computer vision, and autonomous robots. Natural language processing, machine learning and deep learning \n",
    "    are subfields of AI.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create similar/plagiarized text\n",
    "    plagiarized_text = \"\"\"\n",
    "    Artificial intelligence, or AI, is a form of intelligence demonstrated by machines, as opposed to the natural \n",
    "    intelligence displayed by humans and animals. Research in AI has been defined as the study of intelligent agents, \n",
    "    which are systems that observe their surroundings and perform actions that increase their chances of achieving \n",
    "    their objectives. The phrase \"artificial intelligence\" was previously used to describe machines that imitate and exhibit \n",
    "    human cognitive abilities associated with the human mind, such as learning and problem-solving. Major AI researchers \n",
    "    have since rejected this definition and now describe AI in terms of rationality and acting rationally.\n",
    "    \n",
    "    Applications of AI include sophisticated search engines, recommendation systems, autonomous content creation, \n",
    "    speech recognition technology, translation services, computer vision systems, and self-governing robots. \n",
    "    NLP, machine learning and deep learning are important subfields of artificial intelligence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create completely different text\n",
    "    different_text = \"\"\"\n",
    "    Climate change is the long-term alteration of temperature and typical weather patterns in a place. \n",
    "    Climate change could refer to a particular location or the planet as a whole. Climate change may cause \n",
    "    weather patterns to be less predictable. These unexpected weather patterns can make it difficult to \n",
    "    maintain and grow crops in regions that rely on farming because expected temperature and rainfall levels \n",
    "    can no longer be relied on. Wildlife populations can also be impacted by climate change. Some species \n",
    "    are migrating to higher elevations or latitudes to find suitable habitat as their home regions grow warmer.\n",
    "    \n",
    "    The greenhouse effect is the way in which heat is trapped close to Earth's surface by greenhouse gases. \n",
    "    These heat-trapping gases can be thought of as a blanket wrapped around Earth, keeping the planet toasty warm. \n",
    "    Without them, Earth would be a frozen world. But in recent decades, humans have added more greenhouse gases, \n",
    "    making the blanket thicker and stronger. The result is global warming and climate change.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create Python code samples\n",
    "    python_code1 = \"\"\"\n",
    "    def factorial(n):\n",
    "        \"\"\"Calculate factorial using recursion\"\"\"\n",
    "        if n == 0 or n == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return n * factorial(n-1)\n",
    "            \n",
    "    def main():\n",
    "        print(\"Factorial calculator\")\n",
    "        num = int(input(\"Enter a number: \"))\n",
    "        result = factorial(num)\n",
    "        print(f\"The factorial of {num} is {result}\")\n",
    "        \n",
    "    if __name__ == \"__main__\":\n",
    "        main()\n",
    "    \"\"\"\n",
    "    \n",
    "    # Similar Python code with variable name changes and comments\n",
    "    python_code2 = \"\"\"\n",
    "    # Function to calculate factorial\n",
    "    def calculate_factorial(number):\n",
    "        # Base case\n",
    "        if number <= 1:\n",
    "            return 1\n",
    "        # Recursive case\n",
    "        else:\n",
    "            return number * calculate_factorial(number-1)\n",
    "            \n",
    "    # Main program\n",
    "    def run_program():\n",
    "        print(\"Welcome to Factorial Calculator\")\n",
    "        user_input = int(input(\"Please enter a number: \"))\n",
    "        answer = calculate_factorial(user_input)\n",
    "        print(f\"The factorial of {user_input} is {answer}\")\n",
    "        \n",
    "    # Entry point\n",
    "    if __name__ == \"__main__\":\n",
    "        run_program()\n",
    "    \"\"\"\n",
    "    \n",
    "    # Completely different Python code\n",
    "    python_code3 = \"\"\"\n",
    "    class BankAccount:\n",
    "        def __init__(self, owner, balance=0):\n",
    "            self.owner = owner\n",
    "            self.balance = balance\n",
    "            \n",
    "        def deposit(self, amount):\n",
    "            if amount > 0:\n",
    "                self.balance += amount\n",
    "                return True\n",
    "            return False\n",
    "            \n",
    "        def withdraw(self, amount):\n",
    "            if 0 < amount <= self.balance:\n",
    "                self.balance -= amount\n",
    "                return True\n",
    "            return False\n",
    "            \n",
    "        def __str__(self):\n",
    "            return f\"Account owner: {self.owner}\\nBalance: ${self.balance}\"\n",
    "            \n",
    "    # Test the BankAccount class\n",
    "    if __name__ == \"__main__\":\n",
    "        account = BankAccount(\"John Doe\", 1000)\n",
    "        print(account)\n",
    "        account.deposit(500)\n",
    "        account.withdraw(200)\n",
    "        print(account)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save text files\n",
    "    with open('test_files/original.txt', 'w') as f:\n",
    "        f.write(original_text)\n",
    "        \n",
    "    with open('test_files/plagiarized.txt', 'w') as f:\n",
    "        f.write(plagiarized_text)\n",
    "        \n",
    "    with open('test_files/different.txt', 'w') as f:\n",
    "        f.write(different_text)\n",
    "    \n",
    "    # Save Python files\n",
    "    with open('test_files/code1.py', 'w') as f:\n",
    "        f.write(python_code1)\n",
    "        \n",
    "    with open('test_files/code2.py', 'w') as f:\n",
    "        f.write(python_code2)\n",
    "        \n",
    "    with open('test_files/code3.py', 'w') as f:\n",
    "        f.write(python_code3)\n",
    "    \n",
    "    print(\"Test files created in 'test_files' directory\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create test files\n",
    "create_test_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test text comparison\n",
    "print(\"Testing text comparison:\")\n",
    "result_text = detect_plagiarism('test_files/original.txt', 'test_files/plagiarized.txt', 'text_text')\n",
    "print(f\"Similarity between original and plagiarized: {result_text['similarity']*100:.2f}%\")\n",
    "if 'semantic_similarity' in result_text:\n",
    "    print(f\"Semantic similarity: {result_text['semantic_similarity']*100:.2f}%\")\n",
    "if 'style_similarity' in result_text:\n",
    "    print(f\"Style similarity: {result_text['style_similarity']*100:.2f}%\")\n",
    "\n",
    "result_diff = detect_plagiarism('test_files/original.txt', 'test_files/different.txt', 'text_text')\n",
    "print(f\"Similarity between original and different text: {result_diff['similarity']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test code comparison (with basic TF-IDF if CodeBERT not available)\n",
    "if os.path.exists('test_files/code1.py') and os.path.exists('test_files/code2.py'):\n",
    "    print(\"\\nTesting code comparison:\")\n",
    "    \n",
    "    # Create temporary zip files for GitHub comparison simulation\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile('test_files/repo1.zip', 'w') as zipf:\n",
    "        zipf.write('test_files/code1.py', arcname='factorial.py')\n",
    "        \n",
    "    with zipfile.ZipFile('test_files/repo2.zip', 'w') as zipf:\n",
    "        zipf.write('test_files/code2.py', arcname='calc_factorial.py')\n",
    "        \n",
    "    with zipfile.ZipFile('test_files/repo3.zip', 'w') as zipf:\n",
    "        zipf.write('test_files/code3.py', arcname='bank_account.py')\n",
    "    \n",
    "    # Compare similar code repos\n",
    "    result_code = compare_github_repos('test_files/repo1.zip', 'test_files/repo2.zip')\n",
    "    print(f\"Similarity between similar code repos: {result_code['similarity']*100:.2f}%\")\n",
    "    print(f\"Matching files: {result_code.get('matching_files', 'N/A')}\")\n",
    "    \n",
    "    # Compare different code repos\n",
    "    result_diff_code = compare_github_repos('test_files/repo1.zip', 'test_files/repo3.zip')\n",
    "    print(f\"Similarity between different code repos: {result_diff_code['similarity']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Display Matching Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def display_matched_sections(result):\n",
    "    \"\"\"Display the matched sections between the files in a readable format\"\"\"\n",
    "    if 'matched_sections' not in result or len(result['matched_sections']) == 0:\n",
    "        print(\"No significant matching sections found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(result['matched_sections'])} significant matching sections:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, section in enumerate(result['matched_sections']):\n",
    "        print(f\"Match #{i+1} (Similarity: {section['similarity']*100:.2f}%)\")\n",
    "        print(\"\\nFile 1:\")\n",
    "        print(section['file1_text'])\n",
    "        print(\"\\nFile 2:\")\n",
    "        print(section['file2_text'])\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display matched sections for text comparison\n",
    "print(\"Matched sections for similar texts:\")\n",
    "try:\n",
    "    display_matched_sections(result_text)\n",
    "except NameError:\n",
    "    print(\"Run the text comparison test first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display matched sections for code comparison\n",
    "print(\"Matched sections for similar code:\")\n",
    "try:\n",
    "    display_matched_sections(result_code)\n",
    "except NameError:\n",
    "    print(\"Run the code comparison test first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion and Advanced Features Summary\n",
    "\n",
    "The advanced plagiarism detection system implements several enhancements over traditional plagiarism checkers:\n",
    "\n",
    "1. **Semantic Understanding** with Sentence-BERT\n",
    "   - Captures meaning beyond exact word matches\n",
    "   - Better at detecting paraphrased content\n",
    "   - More accurate similarity scores for related content\n",
    "\n",
    "2. **OCR for Scanned Documents** with EasyOCR\n",
    "   - Can process scanned PDFs that traditional text extractors can't handle\n",
    "   - Works with image-based content\n",
    "\n",
    "3. **Code Understanding** with CodeBERT\n",
    "   - Specialized for programming languages\n",
    "   - Can detect code similarities despite variable renaming and formatting changes\n",
    "   - Language-aware preprocessing\n",
    "\n",
    "4. **Stylometric Analysis**\n",
    "   - Detects writing style similarities\n",
    "   - Helps identify content from the same author\n",
    "   - Useful for detecting ghostwriting and unauthorized collaboration\n",
    "\n",
    "5. **Graceful Degradation**\n",
    "   - Falls back to simpler methods when advanced models aren't available\n",
    "   - Ensures functionality in environments with limited resources\n",
    "\n",
    "These advanced features provide more accurate and comprehensive plagiarism detection across different content types and scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}